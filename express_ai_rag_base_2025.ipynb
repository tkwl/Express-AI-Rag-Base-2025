{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "",
    "\n",
    "<h1 align=\"center\" id=\"heading\">Express AI Rag Base 2025</h1>\n",
    "\n",
    "We will be implementing the best-practice tools for RAG in 2025.\n",
    "\n",
    "We'll go over how to build a solid baseline RAG application (essentially what is tablestakes as of March 26th, 2025). \n",
    "\n",
    "This is by no means the *end* of the line for RAG - but it is what you should be building *at minimum* as you create RAG applications. \n",
    "\n",
    "Let's start with a big ole wall of environment variables.\n",
    "\n",
    "> NOTE: We're using cloud endpoints for demonstration purposes, you could deploy these models locally to avoid transmitting data outside your org. if required. There is one exception, with Cohere's Rerank model - an open-source alternative will be provided in the section for Reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getpass\n",
    "\n",
    "os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass(\"Provide your Together API Key: \")\n",
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Provide your Cohere API Key: \")\n",
    "os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Provide your NVIDIA API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also select a best practice visibility and monitoring tool LangSmith, assuming you are using the LangX ecosystem for building the RAG application. \n",
    "\n",
    "> NOTE: You should substitute this tool with [Arize Phoenix](https://docs.arize.com/phoenix) if you are not using the LangX ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"RAG-In-Practice-2025\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Provide your Langsmith API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion of Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we're not using LangChain's default splitters. While this example is simplified - the idea holds true: \n",
    "\n",
    "Chunking/splitting is something that should be done in a thought-out way, relying on base chunking strategies is not a suggested practice. \n",
    "\n",
    "Since our data is quotations separated by `\\n\\n` - we'll build a simple function that splits our data into coherent chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(doc_path: str) -> list:\n",
    "    # Read the document\n",
    "    with open(doc_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split the document by double newlines\n",
    "    chunks = content.split('\\n\\n')\n",
    "    \n",
    "    # Remove any empty chunks\n",
    "    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_vinci_documents = split_docs(\"./data/da_vinci.txt\")\n",
    "len(da_vinci_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three classes of people: those who see. Those who see when they are shown. Those who do not see.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da_vinci_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "henry_david_thoreau_documents = split_docs(\"./data/henry_david_thoreau.txt\")\n",
    "len(henry_david_thoreau_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. “All good things are wild and free.”'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "henry_david_thoreau_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see - our Thoreau quotations have distracting characters - we'll clean those. \n",
    "\n",
    "> NOTE: Data quality is still incredibly important - again, this is a simple example - but it's meant to demonstate that you should care about your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_hdt_quotations(list_of_quotations):\n",
    "    cleaned_quotations = []\n",
    "    \n",
    "    for quotation in list_of_quotations:\n",
    "        without_numbering = re.sub(r'^\\d+\\.\\s+', '', quotation)\n",
    "        \n",
    "        cleaned_quote = without_numbering.strip('”')\n",
    "        if cleaned_quote.startswith('“'):\n",
    "            cleaned_quote = cleaned_quote[1:]\n",
    "        \n",
    "        cleaned_quotations.append(cleaned_quote)\n",
    "    \n",
    "    return cleaned_quotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All good things are wild and free.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "henry_david_thoreau_documents_clean = clean_hdt_quotations(henry_david_thoreau_documents)\n",
    "henry_david_thoreau_documents_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model\n",
    "\n",
    "We'll be using [llama-3.2-nv-embedqa-1b-v2](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2) as our Embedding Model today - it's fast, efficient, has long context, supports multiple languages, has incredibly high scores for retrieval tasks, and can be securely and locally hosted as a NIM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "nvidia_embeddings = NVIDIAEmbeddings(\n",
    "  model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", \n",
    "  truncate=\"NONE\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nvidia_embeddings.embed_query(\"Hello, there!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vector Store - Qdrant\n",
    "\n",
    "Qdrant is our recommended Vector Database as it serves as a tool that you can start with, and will scale with you as your needs grow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=\"quote_collection\",\n",
    "    vectors_config=VectorParams(size=2048, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"quote_collection\",\n",
    "    embedding=nvidia_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Notice how we inject metadata into Qdrant - this can be leveraged directly or indirectly as we'll see when we use the application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = []\n",
    "uuids = []\n",
    "\n",
    "for i, quote in enumerate(henry_david_thoreau_documents_clean):\n",
    "    document = Document(\n",
    "        page_content=quote,\n",
    "        metadata={\"author\": f\"Henry David Thoreau\"},\n",
    "    )\n",
    "    uuid = str(uuid4())\n",
    "    documents.append(document)\n",
    "    uuids.append(uuid)\n",
    "\n",
    "_ = vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "uuids = []\n",
    "\n",
    "for i, quote in enumerate(da_vinci_documents):\n",
    "    document = Document(\n",
    "        page_content=quote,\n",
    "        metadata={\"author\": f\"Leonardo da Vinci\"},\n",
    "    )\n",
    "    uuid = str(uuid4())\n",
    "    documents.append(document)\n",
    "    uuids.append(uuid)\n",
    "\n",
    "_ = vector_store.add_documents(documents=documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Retriever\n",
    "\n",
    "We are going to recommend that for retrieval you begin with: \n",
    "\n",
    "1. Hybrid Retrieval (Dense + Keyword)\n",
    "2. Reranking \n",
    "\n",
    "While methods like HyDE can be leveraged to great effect in some situations (as was shown in [this paper](https://arxiv.org/pdf/2407.01219)) - they're not as universally useful as the above flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Retrieval\n",
    "\n",
    "We'll combine our dense retreival (LLM Embeddings) with BM25 and combine them with reciprocal rank-fusion (rRF) to produce a final list. \n",
    "\n",
    "> NOTE: We are choosing 5 pieces of context from both retrieval processes - this is an adjustable hyperparameter. The number should be larger than your final desired number of documents since the Reranking process can be used to filter out the lowest scoring options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    texts=henry_david_thoreau_documents_clean+da_vinci_documents,\n",
    ")\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[\n",
    "        vector_store.as_retriever(search_kwargs={\"k\": 5}), \n",
    "        bm25_retriever\n",
    "    ],\n",
    "    weights=[0.5, 0.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking\n",
    "\n",
    "Reranking is an extremely important piece of the puzzle - as it allows us to more deeply search through a subset of our documents. \n",
    "\n",
    "> NOTE: As an open-source alternative, you could use [llama-3.2-nv-rerankqa-1b-v2](https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2), which has the same benefits as the Embedding Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "cohere_rerank = CohereRerank(\n",
    "    model=\"rerank-english-v3.0\",\n",
    "    top_n=5,\n",
    ")\n",
    "\n",
    "reranker = ContextualCompressionRetriever(\n",
    "    base_compressor=cohere_rerank, base_retriever=hybrid_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Prompt\n",
    "\n",
    "Prompts are still crucial - and while this prompt works well for this use-case, time should still be spent on exactly how you're prompting your LLM based on a number of factors. (generator model being used, number of documents retrieved, etc.)\n",
    "\n",
    "This process is, like data ingestion, something to spend time and effort on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "#CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{query}\n",
    "\n",
    "Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context response with \"I don't know\"\n",
    "\"\"\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", RAG_TEMPLATE)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "While we're recommending [meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) as the \"default best RAG model\" - this is another choice that is use-case, domain, and cost based. \n",
    "\n",
    "Right-sizing your model is still a difficult process in 2025 - but for most use-cases where you need a good model and can afford/support Llama 3.3 70B Instruct - you should find it extremely effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether\n",
    "\n",
    "llm = ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Henry David Thoreau, an American philosopher and writer, emphasized the importance of solitude in his works, particularly in his book \"Walden; or, Life in the Woods\" (1854). Thoreau believed that solitude was essential for personal growth, self-reflection, and spiritual development. He argued that by spending time alone in nature, individuals could escape the distractions and superficialities of modern life, and instead, focus on their inner lives and connect with the natural world.\\n\\nThoreau wrote, \"I never found a companion that was so companionable as solitude.\" He believed that solitude allowed him to think more deeply, to observe the world around him more closely, and to cultivate a sense of inner peace and contentment. He also saw solitude as a means of simplifying one\\'s life, of stripping away unnecessary distractions and focusing on what was truly essential.\\n\\nIn \"Walden,\" Thoreau described his experience of living alone in a cabin near Walden Pond, where he spent two years, two months, and two days in solitude. During this time, he kept a journal, observed the natural world, and reflected on his own life and values. He wrote, \"I went to the woods because I wished to live deliberately, to front only the essential facts of life, and see if I could not learn what it had to teach, and not, when I came to die, discover that I had not lived.\"\\n\\nThoreau\\'s emphasis on the importance of solitude has inspired many people to reevaluate their priorities and seek out time for quiet reflection and contemplation. His ideas continue to influence contemporary thought on the value of solitude, simplicity, and living in harmony with nature.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 344, 'prompt_tokens': 47, 'total_tokens': 391, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Llama-3.3-70B-Instruct-Turbo', 'system_fingerprint': None, 'id': 'nnaPAJ3-3NKUce-9267e052bb9dab66', 'finish_reason': 'stop', 'logprobs': None}, id='run-020737af-5dbb-4748-bf20-7370cd199700-0', usage_metadata={'input_tokens': 47, 'output_tokens': 344, 'total_tokens': 391, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What did Thoreau say about the importance of solitude?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RAG Graph\n",
    "\n",
    "We continue to suggest people to use LangGraph in 2025. Notice that LCEL is largely removed as the core way to interact with the LangX ecosystem - as we prefer to interact with it through LangGraph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: There is a subtle best-practice in the next cell, which is to return your contexts in ascending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: RAGState) -> RAGState:\n",
    "  retrieved_docs = reranker.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs[::-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "  generation_chain = chat_prompt | llm | StrOutputParser()\n",
    "  response = generation_chain.invoke({\"query\" : state[\"question\"], \"context\" : state[\"context\"]})\n",
    "  return {\"response\" : response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(RAGState)\n",
    "graph_builder = graph_builder.add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Present in Documents\n",
    "\n",
    "In this case, we know the question should not be answerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({\"question\" : \"Why do birds sing?\"})\n",
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong Author\n",
    "\n",
    "In this case, we know Da Vinci didn't have quotes about solitude in the corpus we used - but Thoreau did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({\"question\" : \"What did Da Vinci have to say about solitude?\"})\n",
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'relevance_score': 1.3109144e-05}, page_content='Life isn’t about finding yourself; it’s about creating yourself. So live the life you imagined.'),\n",
       " Document(metadata={'author': 'Leonardo da Vinci', '_id': 'ffa4abb0-958c-4435-8eae-d53264bdb095', '_collection_name': 'quote_collection', 'relevance_score': 1.7778551e-05}, page_content=\"Now you see that the hope and the desire of returning home and to one's former state is like the moth to the light, and that the man who with constant longing awaits with joy each new spring time, each new summer, each new month and new year — deeming that the things he longs for are ever too late in coming — does not perceive that he is longing for his own destruction. But this desire is the very quintessence, the spirit of the elements, which finding itself imprisoned with the soul is ever longing to return from the human body to its giver. And you must know that this same longing is that quintessence, inseparable from nature, and that man is the image of the world.\"),\n",
       " Document(metadata={'author': 'Leonardo da Vinci', '_id': 'c63e626b-7566-489f-9e23-cb5d0e3c7e4d', '_collection_name': 'quote_collection', 'relevance_score': 0.00017952797}, page_content='If you meet with any one who is virtuous do not drive him from you; do him honour, so that he may not have to flee from you and be reduced to hiding in hermitages, or caves or other solitary places to escape from your treachery; if there is such an one among you do him honour, for these are our Saints upon earth; these are they who deserve statues from us, and images...'),\n",
       " Document(metadata={'author': 'Leonardo da Vinci', '_id': '205bfe33-2d97-4de8-85c8-ed4b33a8803c', '_collection_name': 'quote_collection', 'relevance_score': 0.00039511538}, page_content='\"Study me, reader, if you delight in me, because on very few occasions shall I return to the world, and because the patience for this profession is found in very few, and only in those who wish to compose things anew. Come, oh men, to see the miracles that such studies will disclose to nature.\"'),\n",
       " Document(metadata={'author': 'Henry David Thoreau', '_id': '1c8c191a-46ae-43a4-8b3b-e723484fea25', '_collection_name': 'quote_collection', 'relevance_score': 0.0027576224}, page_content='I find it wholesome to be alone the greater part of the time. To be in company, even with the best, is soon wearisome and dissipating. I love to be alone. I never found the companion that was so companionable as solitude.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present in Documents\n",
    "\n",
    "Of course, the system should work for cases where the documentation exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thoreau said, \"I find it wholesome to be alone the greater part of the time. To be in company, even with the best, is soon wearisome and dissipating. I love to be alone. I never found the companion that was so companionable as solitude.\"'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({\"question\" : \"What did Thoreau have to say about solitude?\"})\n",
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'relevance_score': 4.5397872e-05}, page_content='Life isn’t about finding yourself; it’s about creating yourself. So live the life you imagined.'),\n",
       " Document(metadata={'author': 'Leonardo da Vinci', '_id': 'c63e626b-7566-489f-9e23-cb5d0e3c7e4d', '_collection_name': 'quote_collection', 'relevance_score': 0.00021152888}, page_content='If you meet with any one who is virtuous do not drive him from you; do him honour, so that he may not have to flee from you and be reduced to hiding in hermitages, or caves or other solitary places to escape from your treachery; if there is such an one among you do him honour, for these are our Saints upon earth; these are they who deserve statues from us, and images...'),\n",
       " Document(metadata={'author': 'Leonardo da Vinci', '_id': '205bfe33-2d97-4de8-85c8-ed4b33a8803c', '_collection_name': 'quote_collection', 'relevance_score': 0.0007321813}, page_content='\"Study me, reader, if you delight in me, because on very few occasions shall I return to the world, and because the patience for this profession is found in very few, and only in those who wish to compose things anew. Come, oh men, to see the miracles that such studies will disclose to nature.\"'),\n",
       " Document(metadata={'author': 'Henry David Thoreau', '_id': '018d9442-4bda-44a1-b6f0-8837f25e02a0', '_collection_name': 'quote_collection', 'relevance_score': 0.001957017}, page_content='I went to the woods because I wished to live deliberately, to front only the essential facts of life, and see if I could not learn what it had to teach, and not, when I came to die, discover that I had not lived.'),\n",
       " Document(metadata={'author': 'Henry David Thoreau', '_id': '1c8c191a-46ae-43a4-8b3b-e723484fea25', '_collection_name': 'quote_collection', 'relevance_score': 0.00809329}, page_content='I find it wholesome to be alone the greater part of the time. To be in company, even with the best, is soon wearisome and dissipating. I love to be alone. I never found the companion that was so companionable as solitude.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesizing Responses\n",
    "\n",
    "Asking about quotes that are similar should allow us to compare and contrast each author's thoughts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, here\\'s a comparison of Da Vinci and Thoreau\\'s thoughts on perception/seeing:\\n\\nDa Vinci\\'s quotes suggest that he believed perception and seeing are closely tied to the functioning of the eye and the process of observation. He stated that \"Drawing is based upon perspective, which is nothing else than a thorough knowledge of the function of the eye\" and \"All our knowledge has its origin in our perceptions.\" He also wondered about the difference in clarity between dreams and wakeful imagination, implying an interest in the mechanisms of perception.\\n\\nThoreau, on the other hand, took a more philosophical approach, saying \"It\\'s not what you look at that matters, it\\'s what you see.\" This quote implies that Thoreau believed seeing is not just a physical act, but also a mental and interpretive one.\\n\\nIn contrast to Da Vinci\\'s focus on the physical aspects of perception, Thoreau\\'s quote suggests that the meaning and significance we derive from what we see are more important than the mere act of looking. While Da Vinci seemed to be interested in the technical and scientific aspects of perception, Thoreau\\'s thought is more concerned with the subjective and interpretive nature of seeing.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = graph.invoke({\"question\" : \"What did Da Vinci and Thoreau say about perception/seeing? Compare and contrast their thoughts.\"})\n",
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution\n",
    "\n",
    "For Evaluating RAG applications in 2025 - we continue to recommend [Ragas](https://github.com/explodinggradients/ragas/tree/main) as a best practice tool. \n",
    "\n",
    "> NOTE: Please see our [Ragas event](https://www.youtube.com/watch?v=bB56BaQIBm4) for more in-depth explanations and examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Provide your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import AspectCritic\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "test_data = {\n",
    "    \"user_input\": \"What did Da Vinci and Thoreau say about perception/seeing? Compare and contrast their thoughts.\",\n",
    "    \"response\": response[\"response\"],\n",
    "}\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "metric = AspectCritic(name=\"fully_answered\",llm=evaluator_llm, definition=\"Verify if the question was answered completely.\")\n",
    "await metric.single_turn_ascore(SingleTurnSample(**test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
